{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data from text files\n",
    "with open('data/reviews.txt', 'r') as f:\n",
    "    reviews = f.read()\n",
    "with open('data/labels.txt', 'r') as f:\n",
    "    labels = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bromwell high is a cartoon comedy . it ran at the same time as some other programs about school life  such as  teachers  . my   years in the teaching profession lead me to believe that bromwell high  s satire is much closer to reality than is  teachers  . the scramble to survive financially  the insightful students who can see right through their pathetic teachers  pomp  the pettiness of the whole situation  all remind me of the schools i knew and their students . when i saw the episode in which a student repeatedly tried to burn down the school  i immediately recalled . . . . . . . . . at . . . . . . . . . . high . a classic line inspector i  m here to sack one of your teachers . student welcome to bromwell high . i expect that many adults of my age think that bromwell high is far fetched . what a pity that it isn  t   \n",
      "story of a man who has unnatural feelings for a pig . starts out with a opening scene that is a terrific example of absurd comedy . a formal orchestra audience is turned into an insane  violent mob by the crazy chantings of it  s singers . unfortunately it stays absurd the whole time with no general narrative eventually making it just too off putting . even those from the era should be turned off . the cryptic dialogue would make shakespeare seem easy to a third grader . on a technical level it  s better than you might think with some good cinematography by future great vilmos zsigmond . future stars sally kirkland and frederic forrest can be seen briefly .  \n",
      "homelessness  or houselessness as george carlin stated  has been an issue for years but never a plan to help those on the street that were once considered human who did everything from going to school  work  or vote for the matter . most people think of the homeless as just a lost cause while worrying about things such as racism  the war on iraq  pressuring kids to succeed  technology  the elections  inflation  or worrying if they  ll be next to end up on the streets .  br    br   but what if y\n",
      "\n",
      "positive\n",
      "negative\n",
      "po\n"
     ]
    }
   ],
   "source": [
    "print(reviews[:2000])\n",
    "print()\n",
    "print(labels[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "\n",
    "print(punctuation)\n",
    "\n",
    "# get rid of punctuation\n",
    "reviews = reviews.lower() # lowercase, standardize\n",
    "all_text = ''.join([c for c in reviews if c not in punctuation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split by new lines and spaces\n",
    "reviews_split = all_text.split('\\n')\n",
    "all_text = ' '.join(reviews_split)\n",
    "\n",
    "# create a list of words\n",
    "words = all_text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bromwell',\n",
       " 'high',\n",
       " 'is',\n",
       " 'a',\n",
       " 'cartoon',\n",
       " 'comedy',\n",
       " 'it',\n",
       " 'ran',\n",
       " 'at',\n",
       " 'the',\n",
       " 'same',\n",
       " 'time',\n",
       " 'as',\n",
       " 'some',\n",
       " 'other',\n",
       " 'programs',\n",
       " 'about',\n",
       " 'school',\n",
       " 'life',\n",
       " 'such',\n",
       " 'as',\n",
       " 'teachers',\n",
       " 'my',\n",
       " 'years',\n",
       " 'in',\n",
       " 'the',\n",
       " 'teaching',\n",
       " 'profession',\n",
       " 'lead',\n",
       " 'me']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_to_int = {word:i+1 for (i, word) in enumerate(set(words))}\n",
    "\n",
    "reviews_ints = []\n",
    "for text in reviews_split:\n",
    "    reviews_ints.append([vocab_to_int[num] for num in text.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words:  74072\n",
      "\n",
      "Tokenized review: \n",
      " [[54344, 71045, 36957, 13135, 25958, 45677, 19006, 51256, 7246, 20586, 33398, 11612, 34985, 51241, 29768, 26911, 45036, 57150, 526, 38058, 34985, 46939, 25715, 51609, 71032, 20586, 28479, 39249, 10544, 36947, 38785, 43713, 47221, 54344, 71045, 27676, 47164, 36957, 66434, 61452, 38785, 4920, 47729, 36957, 46939, 20586, 59592, 38785, 19571, 644, 20586, 47874, 18950, 24970, 21302, 8134, 35822, 67532, 10508, 72550, 46939, 51357, 20586, 63103, 31340, 20586, 25922, 43958, 53918, 70616, 36947, 31340, 20586, 56557, 63840, 14569, 40895, 10508, 18950, 29797, 63840, 7681, 20586, 16073, 71032, 1470, 13135, 2603, 47022, 61168, 38785, 12588, 29584, 20586, 57150, 63840, 73917, 50680, 7246, 71045, 13135, 59290, 63842, 54834, 63840, 45418, 36116, 38785, 17766, 14420, 31340, 68863, 46939, 2603, 2184, 38785, 54344, 71045, 63840, 52525, 47221, 36386, 28341, 31340, 25715, 72730, 60047, 47221, 54344, 71045, 36957, 5398, 47688, 62079, 13135, 41617, 47221, 19006, 64034, 12864]]\n"
     ]
    }
   ],
   "source": [
    "# stats about vocabulary\n",
    "print('Unique words: ', len((vocab_to_int)))  # should ~ 74000+\n",
    "print()\n",
    "\n",
    "# print tokens in first review\n",
    "print('Tokenized review: \\n', reviews_ints[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " ...]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_labels = [1 if ch==\"positive\" else 0 if ch==\"negative\" else None for ch in labels.split(\"\\n\")]\n",
    "encoded_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-length reviews: 1\n",
      "Maximum review length: 2514\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# outlier review stats\n",
    "review_lens = Counter([len(x) for x in reviews_ints])\n",
    "print(\"Zero-length reviews: {}\".format(review_lens[0]))\n",
    "print(\"Maximum review length: {}\".format(max(review_lens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews before removing outliers:  25001\n",
      "Number of labels before removing outliers:  25001\n",
      "Number of reviews after removing outliers:  25000\n",
      "Number of labels before removing outliers:  25000\n"
     ]
    }
   ],
   "source": [
    "print('Number of reviews before removing outliers: ', len(reviews_ints))\n",
    "print('Number of labels before removing outliers: ', len(encoded_labels))\n",
    "\n",
    "# remove any reviews/labels with zero length from the reviews_ints list.\n",
    "\n",
    "reviews_ints = [text for text in reviews_ints if len(text)!=0]\n",
    "encoded_labels = [ele for ele in encoded_labels if ele!=None]\n",
    "\n",
    "print('Number of reviews after removing outliers: ', len(reviews_ints))\n",
    "print('Number of labels before removing outliers: ', len(encoded_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[54344,\n",
       "  71045,\n",
       "  36957,\n",
       "  13135,\n",
       "  25958,\n",
       "  45677,\n",
       "  19006,\n",
       "  51256,\n",
       "  7246,\n",
       "  20586,\n",
       "  33398,\n",
       "  11612,\n",
       "  34985,\n",
       "  51241,\n",
       "  29768,\n",
       "  26911,\n",
       "  45036,\n",
       "  57150,\n",
       "  526,\n",
       "  38058,\n",
       "  34985,\n",
       "  46939,\n",
       "  25715,\n",
       "  51609,\n",
       "  71032,\n",
       "  20586,\n",
       "  28479,\n",
       "  39249,\n",
       "  10544,\n",
       "  36947,\n",
       "  38785,\n",
       "  43713,\n",
       "  47221,\n",
       "  54344,\n",
       "  71045,\n",
       "  27676,\n",
       "  47164,\n",
       "  36957,\n",
       "  66434,\n",
       "  61452,\n",
       "  38785,\n",
       "  4920,\n",
       "  47729,\n",
       "  36957,\n",
       "  46939,\n",
       "  20586,\n",
       "  59592,\n",
       "  38785,\n",
       "  19571,\n",
       "  644,\n",
       "  20586,\n",
       "  47874,\n",
       "  18950,\n",
       "  24970,\n",
       "  21302,\n",
       "  8134,\n",
       "  35822,\n",
       "  67532,\n",
       "  10508,\n",
       "  72550,\n",
       "  46939,\n",
       "  51357,\n",
       "  20586,\n",
       "  63103,\n",
       "  31340,\n",
       "  20586,\n",
       "  25922,\n",
       "  43958,\n",
       "  53918,\n",
       "  70616,\n",
       "  36947,\n",
       "  31340,\n",
       "  20586,\n",
       "  56557,\n",
       "  63840,\n",
       "  14569,\n",
       "  40895,\n",
       "  10508,\n",
       "  18950,\n",
       "  29797,\n",
       "  63840,\n",
       "  7681,\n",
       "  20586,\n",
       "  16073,\n",
       "  71032,\n",
       "  1470,\n",
       "  13135,\n",
       "  2603,\n",
       "  47022,\n",
       "  61168,\n",
       "  38785,\n",
       "  12588,\n",
       "  29584,\n",
       "  20586,\n",
       "  57150,\n",
       "  63840,\n",
       "  73917,\n",
       "  50680,\n",
       "  7246,\n",
       "  71045,\n",
       "  13135,\n",
       "  59290,\n",
       "  63842,\n",
       "  54834,\n",
       "  63840,\n",
       "  45418,\n",
       "  36116,\n",
       "  38785,\n",
       "  17766,\n",
       "  14420,\n",
       "  31340,\n",
       "  68863,\n",
       "  46939,\n",
       "  2603,\n",
       "  2184,\n",
       "  38785,\n",
       "  54344,\n",
       "  71045,\n",
       "  63840,\n",
       "  52525,\n",
       "  47221,\n",
       "  36386,\n",
       "  28341,\n",
       "  31340,\n",
       "  25715,\n",
       "  72730,\n",
       "  60047,\n",
       "  47221,\n",
       "  54344,\n",
       "  71045,\n",
       "  36957,\n",
       "  5398,\n",
       "  47688,\n",
       "  62079,\n",
       "  13135,\n",
       "  41617,\n",
       "  47221,\n",
       "  19006,\n",
       "  64034,\n",
       "  12864],\n",
       " [72391,\n",
       "  31340,\n",
       "  13135,\n",
       "  73267,\n",
       "  24970,\n",
       "  16973,\n",
       "  9231,\n",
       "  4296,\n",
       "  55842,\n",
       "  13135,\n",
       "  63046,\n",
       "  23283,\n",
       "  31036,\n",
       "  49184,\n",
       "  13135,\n",
       "  53799,\n",
       "  49059,\n",
       "  47221,\n",
       "  36957,\n",
       "  13135,\n",
       "  60365,\n",
       "  22386,\n",
       "  31340,\n",
       "  59786,\n",
       "  45677,\n",
       "  13135,\n",
       "  23357,\n",
       "  48831,\n",
       "  10570,\n",
       "  36957,\n",
       "  3141,\n",
       "  41876,\n",
       "  42627,\n",
       "  54619,\n",
       "  60222,\n",
       "  11982,\n",
       "  38928,\n",
       "  20586,\n",
       "  64162,\n",
       "  62262,\n",
       "  31340,\n",
       "  19006,\n",
       "  27676,\n",
       "  17767,\n",
       "  73959,\n",
       "  19006,\n",
       "  19764,\n",
       "  59786,\n",
       "  20586,\n",
       "  25922,\n",
       "  11612,\n",
       "  49184,\n",
       "  10640,\n",
       "  6166,\n",
       "  44083,\n",
       "  56760,\n",
       "  5846,\n",
       "  19006,\n",
       "  34512,\n",
       "  63246,\n",
       "  60360,\n",
       "  37391,\n",
       "  45213,\n",
       "  60616,\n",
       "  8434,\n",
       "  20586,\n",
       "  1630,\n",
       "  62605,\n",
       "  23200,\n",
       "  3141,\n",
       "  60360,\n",
       "  20586,\n",
       "  26993,\n",
       "  54971,\n",
       "  71963,\n",
       "  17558,\n",
       "  56267,\n",
       "  19629,\n",
       "  22643,\n",
       "  38785,\n",
       "  13135,\n",
       "  51535,\n",
       "  73416,\n",
       "  52121,\n",
       "  13135,\n",
       "  17645,\n",
       "  48622,\n",
       "  19006,\n",
       "  27676,\n",
       "  70394,\n",
       "  47729,\n",
       "  73781,\n",
       "  53775,\n",
       "  60047,\n",
       "  49184,\n",
       "  51241,\n",
       "  58250,\n",
       "  65058,\n",
       "  38928,\n",
       "  5747,\n",
       "  26807,\n",
       "  74070,\n",
       "  56340,\n",
       "  5747,\n",
       "  15398,\n",
       "  25458,\n",
       "  42312,\n",
       "  40895,\n",
       "  31261,\n",
       "  43567,\n",
       "  21302,\n",
       "  23200,\n",
       "  44240,\n",
       "  53169],\n",
       " [2521,\n",
       "  7697,\n",
       "  24253,\n",
       "  34985,\n",
       "  29795,\n",
       "  10703,\n",
       "  62190,\n",
       "  16973,\n",
       "  59577,\n",
       "  42627,\n",
       "  18991,\n",
       "  55842,\n",
       "  51609,\n",
       "  43893,\n",
       "  48134,\n",
       "  13135,\n",
       "  64425,\n",
       "  38785,\n",
       "  36399,\n",
       "  60616,\n",
       "  52121,\n",
       "  20586,\n",
       "  22369,\n",
       "  47221,\n",
       "  10254,\n",
       "  63439,\n",
       "  22452,\n",
       "  32208,\n",
       "  24970,\n",
       "  72181,\n",
       "  30871,\n",
       "  8434,\n",
       "  61187,\n",
       "  38785,\n",
       "  57150,\n",
       "  36938,\n",
       "  7697,\n",
       "  26950,\n",
       "  55842,\n",
       "  20586,\n",
       "  70245,\n",
       "  4721,\n",
       "  14514,\n",
       "  60047,\n",
       "  31340,\n",
       "  20586,\n",
       "  2453,\n",
       "  34985,\n",
       "  34512,\n",
       "  13135,\n",
       "  20334,\n",
       "  20823,\n",
       "  44266,\n",
       "  46793,\n",
       "  45036,\n",
       "  54353,\n",
       "  38058,\n",
       "  34985,\n",
       "  5712,\n",
       "  20586,\n",
       "  56455,\n",
       "  52121,\n",
       "  71431,\n",
       "  21649,\n",
       "  44530,\n",
       "  38785,\n",
       "  15678,\n",
       "  4030,\n",
       "  20586,\n",
       "  42426,\n",
       "  51614,\n",
       "  7697,\n",
       "  46793,\n",
       "  47969,\n",
       "  34457,\n",
       "  4060,\n",
       "  23200,\n",
       "  45234,\n",
       "  38785,\n",
       "  23710,\n",
       "  41110,\n",
       "  52121,\n",
       "  20586,\n",
       "  65120,\n",
       "  16949,\n",
       "  16949,\n",
       "  43893,\n",
       "  62079,\n",
       "  47969,\n",
       "  73781,\n",
       "  10254,\n",
       "  67444,\n",
       "  13135,\n",
       "  687,\n",
       "  38785,\n",
       "  13740,\n",
       "  52121,\n",
       "  20586,\n",
       "  65120,\n",
       "  55842,\n",
       "  13135,\n",
       "  32650,\n",
       "  53274,\n",
       "  20586,\n",
       "  36522,\n",
       "  73781,\n",
       "  63439,\n",
       "  874,\n",
       "  8434,\n",
       "  13135,\n",
       "  71879,\n",
       "  20586,\n",
       "  615,\n",
       "  42154,\n",
       "  13135,\n",
       "  44689,\n",
       "  24646,\n",
       "  52121,\n",
       "  20586,\n",
       "  26890,\n",
       "  13135,\n",
       "  70246,\n",
       "  40895,\n",
       "  30871,\n",
       "  73781,\n",
       "  63439,\n",
       "  58555,\n",
       "  38785,\n",
       "  8134,\n",
       "  62079,\n",
       "  19006,\n",
       "  27676,\n",
       "  40024,\n",
       "  38785,\n",
       "  23200,\n",
       "  2453,\n",
       "  47221,\n",
       "  36957,\n",
       "  36917,\n",
       "  10129,\n",
       "  27676,\n",
       "  22599,\n",
       "  16949,\n",
       "  16949,\n",
       "  10289,\n",
       "  25932,\n",
       "  24970,\n",
       "  34531,\n",
       "  24970,\n",
       "  15398,\n",
       "  34985,\n",
       "  10129,\n",
       "  54456,\n",
       "  13135,\n",
       "  55126,\n",
       "  73267,\n",
       "  24970,\n",
       "  16973,\n",
       "  30871,\n",
       "  71032,\n",
       "  20586,\n",
       "  37097,\n",
       "  35949,\n",
       "  752,\n",
       "  38785,\n",
       "  17558,\n",
       "  13135,\n",
       "  687,\n",
       "  49184,\n",
       "  13135,\n",
       "  67883,\n",
       "  11943,\n",
       "  58856,\n",
       "  46660,\n",
       "  38785,\n",
       "  8134,\n",
       "  47969,\n",
       "  51491,\n",
       "  21302,\n",
       "  13740,\n",
       "  71032,\n",
       "  20586,\n",
       "  65120,\n",
       "  55842,\n",
       "  29887,\n",
       "  28969,\n",
       "  53274,\n",
       "  20586,\n",
       "  36522,\n",
       "  47969,\n",
       "  10129,\n",
       "  9827,\n",
       "  51491,\n",
       "  21302,\n",
       "  17299,\n",
       "  62079,\n",
       "  51491,\n",
       "  54581,\n",
       "  49184,\n",
       "  13135,\n",
       "  5747,\n",
       "  22122,\n",
       "  31340,\n",
       "  5846,\n",
       "  51346,\n",
       "  45007,\n",
       "  20586,\n",
       "  687,\n",
       "  27676,\n",
       "  52121,\n",
       "  68637,\n",
       "  10129,\n",
       "  36957,\n",
       "  19086,\n",
       "  52121,\n",
       "  20586,\n",
       "  22369,\n",
       "  49184,\n",
       "  13135,\n",
       "  58223,\n",
       "  52121,\n",
       "  10798,\n",
       "  27006,\n",
       "  38785,\n",
       "  60568,\n",
       "  10798,\n",
       "  52726,\n",
       "  63434,\n",
       "  68637,\n",
       "  51491,\n",
       "  21302,\n",
       "  12864,\n",
       "  12354,\n",
       "  60360,\n",
       "  20586,\n",
       "  53715,\n",
       "  51491,\n",
       "  27676,\n",
       "  67444,\n",
       "  20586,\n",
       "  17196,\n",
       "  31011,\n",
       "  38928,\n",
       "  13135,\n",
       "  67280,\n",
       "  17755,\n",
       "  19006,\n",
       "  27676,\n",
       "  50498,\n",
       "  52121,\n",
       "  10798,\n",
       "  40503,\n",
       "  68637,\n",
       "  10129,\n",
       "  40155,\n",
       "  29768,\n",
       "  72052,\n",
       "  21467,\n",
       "  13135,\n",
       "  48914,\n",
       "  38928,\n",
       "  20586,\n",
       "  10339,\n",
       "  31340,\n",
       "  27414,\n",
       "  57726,\n",
       "  54298,\n",
       "  19491,\n",
       "  42627,\n",
       "  49868,\n",
       "  54880,\n",
       "  24970,\n",
       "  22545,\n",
       "  57162,\n",
       "  12555,\n",
       "  66380,\n",
       "  29090,\n",
       "  71879,\n",
       "  40895,\n",
       "  29090,\n",
       "  33602,\n",
       "  62595,\n",
       "  53697,\n",
       "  58995,\n",
       "  40895,\n",
       "  37210,\n",
       "  46934,\n",
       "  68779,\n",
       "  24970,\n",
       "  19436,\n",
       "  69535,\n",
       "  12589,\n",
       "  38785,\n",
       "  20586,\n",
       "  65120,\n",
       "  34457,\n",
       "  56991,\n",
       "  65112,\n",
       "  10129,\n",
       "  64034,\n",
       "  12864,\n",
       "  51491,\n",
       "  27676,\n",
       "  72503,\n",
       "  12589,\n",
       "  38785,\n",
       "  5625,\n",
       "  47206,\n",
       "  27331,\n",
       "  40024,\n",
       "  51491,\n",
       "  63439,\n",
       "  72181,\n",
       "  29797,\n",
       "  32039,\n",
       "  55126,\n",
       "  68637,\n",
       "  19006,\n",
       "  27676,\n",
       "  25441,\n",
       "  7697,\n",
       "  30629,\n",
       "  40354,\n",
       "  7697,\n",
       "  23200,\n",
       "  46909,\n",
       "  16949,\n",
       "  16949,\n",
       "  44266,\n",
       "  20586,\n",
       "  31685,\n",
       "  8339,\n",
       "  49454,\n",
       "  27414,\n",
       "  40895,\n",
       "  10129,\n",
       "  17057,\n",
       "  12864,\n",
       "  56849,\n",
       "  38785,\n",
       "  44464,\n",
       "  63840,\n",
       "  8383,\n",
       "  526,\n",
       "  3759,\n",
       "  38785,\n",
       "  23200,\n",
       "  14420,\n",
       "  31340,\n",
       "  10289,\n",
       "  25932,\n",
       "  4356,\n",
       "  19708,\n",
       "  68637,\n",
       "  71606,\n",
       "  38785,\n",
       "  32039,\n",
       "  13135,\n",
       "  45677,\n",
       "  19006,\n",
       "  54749,\n",
       "  13135,\n",
       "  33608,\n",
       "  65639,\n",
       "  41819,\n",
       "  38785,\n",
       "  10798,\n",
       "  35011,\n",
       "  36938,\n",
       "  38058,\n",
       "  34985,\n",
       "  27897,\n",
       "  11571,\n",
       "  27846,\n",
       "  49579,\n",
       "  7697,\n",
       "  33249,\n",
       "  55842,\n",
       "  20586,\n",
       "  70245,\n",
       "  38785,\n",
       "  17315,\n",
       "  62079,\n",
       "  19006,\n",
       "  27676,\n",
       "  40024,\n",
       "  41733,\n",
       "  34747,\n",
       "  33123,\n",
       "  12555,\n",
       "  66380,\n",
       "  19006,\n",
       "  20586,\n",
       "  45234,\n",
       "  33208,\n",
       "  7697,\n",
       "  52121,\n",
       "  20586,\n",
       "  29768,\n",
       "  34631,\n",
       "  5846,\n",
       "  13135,\n",
       "  16208,\n",
       "  687,\n",
       "  40024,\n",
       "  53918,\n",
       "  55126,\n",
       "  14514,\n",
       "  17299,\n",
       "  29797,\n",
       "  34457,\n",
       "  22704,\n",
       "  12864,\n",
       "  2141,\n",
       "  62079,\n",
       "  38785,\n",
       "  17299,\n",
       "  49184,\n",
       "  10508,\n",
       "  10181,\n",
       "  34272,\n",
       "  34457,\n",
       "  62605,\n",
       "  69364,\n",
       "  19006,\n",
       "  38785,\n",
       "  20586,\n",
       "  2453,\n",
       "  38524,\n",
       "  31340,\n",
       "  45298,\n",
       "  19006,\n",
       "  40024,\n",
       "  72480,\n",
       "  10181,\n",
       "  16949,\n",
       "  16949,\n",
       "  7697,\n",
       "  34272,\n",
       "  28087,\n",
       "  55605,\n",
       "  45925,\n",
       "  41770,\n",
       "  73781,\n",
       "  38785,\n",
       "  36399,\n",
       "  63066]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_ints[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros((3, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_features(reviews_ints, seq_length):\n",
    "\n",
    "    features = np.zeros([len(reviews_ints), seq_length])\n",
    "\n",
    "    for i, review in enumerate(reviews_ints):\n",
    "        if len(review) >= seq_length:\n",
    "            features[i, :] = review[:seq_length]\n",
    "        else:\n",
    "            ind = seq_length - len(review)\n",
    "            features[i, ind:] = review[:]\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
      " [    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
      " [ 2521.  7697. 24253. 34985. 29795. 10703. 62190. 16973. 59577. 42627.]\n",
      " [39576. 23283. 34985. 13135. 38556. 72453. 27372. 62490. 36957. 33533.]\n",
      " [    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
      " [    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
      " [    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
      " [    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
      " [    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
      " [29797. 63840. 35721.  9358. 25715. 63163. 10046. 36947. 16572. 38785.]\n",
      " [    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
      " [    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
      " [    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
      " [20586. 46241. 45285. 38928. 13135. 72453. 57868. 56154. 71032. 21426.]\n",
      " [71032. 28087. 69155. 72663. 13897.  8129. 44911. 52121. 67888. 48942.]\n",
      " [    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
      " [20586. 67676. 20781. 34055.   465. 18870. 27587. 38004. 37457. 64109.]\n",
      " [    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
      " [73781.  2141. 34055.   465.  8231. 36138. 10357. 36957. 25261. 25178.]\n",
      " [20586. 55605. 36957. 14107. 66621. 36957. 10640. 29768. 46504. 38785.]\n",
      " [29797. 63840. 57101. 36690. 20764. 22182. 72391. 63840. 35721. 12103.]\n",
      " [28087. 55605. 36957. 14420. 44605. 56297. 71492. 56616. 55017. 36957.]\n",
      " [    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
      " [    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
      " [66621. 19436. 36386. 29231. 10547. 71032. 20586. 55350. 31340. 73267.]\n",
      " [    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
      " [63840. 10510. 20586. 67676. 20781.  3493. 66434. 19006. 27676. 14420.]\n",
      " [    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
      " [    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
      " [    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.]]\n"
     ]
    }
   ],
   "source": [
    "# Test your implementation!\n",
    "\n",
    "seq_length = 200\n",
    "\n",
    "features = pad_features(reviews_ints, seq_length=seq_length)\n",
    "\n",
    "## test statements - do not change - ##\n",
    "assert len(features)==len(reviews_ints), \"Your features should have as many rows as reviews.\"\n",
    "assert len(features[0])==seq_length, \"Each feature row should contain seq_length values.\"\n",
    "\n",
    "# print first 10 values of the first 30 batches \n",
    "print(features[:30,:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_frac = 0.8\n",
    "test_frac = 0.1\n",
    "\n",
    "labels = np.array(encoded_labels)\n",
    "\n",
    "num_train = int(len(features)*train_frac)\n",
    "num_test = int(len(features)*test_frac)\n",
    "\n",
    "train_x = features[:num_train, :]\n",
    "train_y = labels[:num_train]\n",
    "\n",
    "val_x = features[num_train:num_train+num_test, :]\n",
    "val_y = labels[num_train:num_train+num_test]\n",
    "\n",
    "test_x = features[num_train+num_test:, :]\n",
    "test_y = labels[num_train+num_test:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20000, 200), (2500, 200), (2500, 200))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape, val_x.shape, test_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20000,), (2500,), (2500,))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.shape, val_y.shape, val_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# create Tensor datasets\n",
    "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "valid_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n",
    "test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
    "\n",
    "# dataloaders\n",
    "batch_size = 50\n",
    "\n",
    "# make sure to SHUFFLE your data\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input size:  torch.Size([50, 200])\n",
      "Sample input: \n",
      " tensor([[    0.,     0.,     0.,  ..., 18229., 19006., 31416.],\n",
      "        [    0.,     0.,     0.,  ..., 23200., 73493., 63246.],\n",
      "        [32641., 13135., 63041.,  ..., 53515., 13135., 55605.],\n",
      "        ...,\n",
      "        [    0.,     0.,     0.,  ..., 31340., 20586.,  1149.],\n",
      "        [63840., 48134., 29588.,  ..., 10798., 51178., 66621.],\n",
      "        [    0.,     0.,     0.,  ..., 36358.,  7246., 53918.]],\n",
      "       dtype=torch.float64)\n",
      "\n",
      "Sample label size:  torch.Size([50])\n",
      "Sample label: \n",
      " tensor([1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "        0, 1], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# obtain one batch of training data\n",
    "dataiter = iter(train_loader)\n",
    "sample_x, sample_y = next(dataiter)\n",
    "\n",
    "print('Sample input size: ', sample_x.size()) # batch_size, seq_length\n",
    "print('Sample input: \\n', sample_x)\n",
    "print()\n",
    "print('Sample label size: ', sample_y.size()) # batch_size\n",
    "print('Sample label: \\n', sample_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74073"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(vocab_to_int) + 1\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(74073, 400)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Embedding(vocab_size, 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class SentimentRNN(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
    "\n",
    "        super(SentimentRNN, self).__init__()\n",
    "\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # embedding and LSTM layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers,\n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "\n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "        # linear and sigmoid layers\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # embedding and lstm_out\n",
    "        x = x.long()\n",
    "        embeds = self.embedding(x)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "\n",
    "        lstm_out = lstm_out[:, -1, :]\n",
    "\n",
    "        # dropout\n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        sig_out = self.sig(out)\n",
    "\n",
    "        return sig_out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "\n",
    "        weight = next(self.parameters()).data\n",
    "\n",
    "        if use_cuda:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentRNN(\n",
      "  (embedding): Embedding(74073, 400)\n",
      "  (lstm): LSTM(400, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
      "  (sig): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocab_to_int) + 1\n",
    "output_size = 1\n",
    "embedding_dim = 400\n",
    "hidden_dim = 256\n",
    "n_layers = 2\n",
    "\n",
    "net = SentimentRNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/4... Step: 100... Loss: 0.548671... Val Loss: 0.694929\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/4... Step: 200... Loss: 0.679210... Val Loss: 0.694042\n",
      "Epoch: 1/4... Step: 300... Loss: 0.673473... Val Loss: 0.635435\n",
      "Epoch: 1/4... Step: 400... Loss: 0.666673... Val Loss: 0.542992\n",
      "Epoch: 2/4... Step: 500... Loss: 0.434374... Val Loss: 0.505172\n",
      "Epoch: 2/4... Step: 600... Loss: 0.529379... Val Loss: 0.447921\n",
      "Epoch: 2/4... Step: 700... Loss: 0.342679... Val Loss: 0.441646\n",
      "Epoch: 2/4... Step: 800... Loss: 0.488859... Val Loss: 0.476216\n",
      "Epoch: 3/4... Step: 900... Loss: 0.149421... Val Loss: 0.517191\n",
      "Epoch: 3/4... Step: 1000... Loss: 0.288185... Val Loss: 0.658518\n",
      "Epoch: 3/4... Step: 1100... Loss: 0.159463... Val Loss: 0.492053\n",
      "Epoch: 3/4... Step: 1200... Loss: 0.244141... Val Loss: 0.479490\n",
      "Epoch: 4/4... Step: 1300... Loss: 0.071966... Val Loss: 0.566402\n",
      "Epoch: 4/4... Step: 1400... Loss: 0.148028... Val Loss: 0.651843\n",
      "Epoch: 4/4... Step: 1500... Loss: 0.252360... Val Loss: 0.578206\n",
      "Epoch: 4/4... Step: 1600... Loss: 0.131678... Val Loss: 0.551360\n"
     ]
    }
   ],
   "source": [
    "epochs = 4\n",
    "\n",
    "counter = 0\n",
    "print_every = 100\n",
    "clip=5 # gradient clipping\n",
    "\n",
    "if use_cuda:\n",
    "    net.cuda()\n",
    "\n",
    "net.train()\n",
    "\n",
    "for e in range(epochs):\n",
    "\n",
    "    h = net.init_hidden(batch_size)\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "\n",
    "        counter += 1\n",
    "\n",
    "        if use_cuda:\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "        h = tuple([each.data for each in h])\n",
    "\n",
    "        # zero accumulated gradient\n",
    "        net.zero_grad()\n",
    "\n",
    "        # get the output from model\n",
    "        output, h = net(inputs, h)\n",
    "\n",
    "        # calculate the loss and perform backprop\n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "\n",
    "        # clip grad norm helps prevent the exploding gradient problem\n",
    "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        # loss stats\n",
    "        if counter % print_every == 0:\n",
    "\n",
    "            val_h = net.init_hidden(batch_size)\n",
    "            val_losses = []\n",
    "            net.eval()\n",
    "            for inputs, labels in valid_loader:\n",
    "\n",
    "                val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "                if use_cuda:\n",
    "                    inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "                output, val_h = net(inputs, val_h)\n",
    "                val_loss = criterion(output.squeeze(), labels.float())\n",
    "\n",
    "                val_losses.append(val_loss.item())\n",
    "\n",
    "            net.train()\n",
    "            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.544\n",
      "Test accuracy: 0.811\n"
     ]
    }
   ],
   "source": [
    "test_losses = []\n",
    "num_correct = 0\n",
    "\n",
    "h = net.init_hidden(batch_size)\n",
    "\n",
    "net.eval()\n",
    "\n",
    "\n",
    "for inputs, labels in test_loader:\n",
    "\n",
    "    h = tuple([each.data for each in h])\n",
    "\n",
    "    if use_cuda:\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "    output, h = net(inputs, h)\n",
    "\n",
    "    # calculate loss\n",
    "    test_loss = criterion(output.squeeze(), labels.float())\n",
    "    test_losses.append(test_loss.item())\n",
    "\n",
    "    # convert output probabilities to predict class (0 or 1)\n",
    "    pred = torch.round(output.squeeze())\n",
    "\n",
    "    # compare pred with true labels\n",
    "    correct_tensor = pred.eq(labels.float().view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.numpy()) if not use_cuda else np.squeeze(correct_tensor.cpu().numpy())\n",
    "    num_correct += np.sum(correct)\n",
    "\n",
    "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
    "\n",
    "# accuracy over all test data\n",
    "test_acc = num_correct/len(test_loader.dataset)\n",
    "print(\"Test accuracy: {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# negative test review\n",
    "test_review_neg = 'The worst movie I have seen; acting was terrible and I want my money back. This movie had bad acting and the dialogue was slow.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[20586, 26000, 18639, 63840, 5731, 44240, 19694, 35721, 7313, 40895, 63840, 45098, 25715, 10181, 45343, 28087, 18639, 874, 14107, 19694, 40895, 20586, 54971, 35721, 3502]]\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "\n",
    "def tokenize_review(test_review):\n",
    "    test_review = test_review.lower() # lowercase\n",
    "    # get rid of punctuation\n",
    "    test_text = ''.join([c for c in test_review if c not in punctuation])\n",
    "\n",
    "    # splitting by spaces\n",
    "    test_words = test_text.split()\n",
    "\n",
    "    # tokens\n",
    "    test_ints = []\n",
    "    test_ints.append([vocab_to_int.get(word, 0) for word in test_words])\n",
    "\n",
    "    return test_ints\n",
    "\n",
    "# test code and generate tokenized review\n",
    "test_ints = tokenize_review(test_review_neg)\n",
    "print(test_ints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "      0.     0.     0.     0.     0. 20586. 26000. 18639. 63840.  5731.\n",
      "  44240. 19694. 35721.  7313. 40895. 63840. 45098. 25715. 10181. 45343.\n",
      "  28087. 18639.   874. 14107. 19694. 40895. 20586. 54971. 35721.  3502.]]\n"
     ]
    }
   ],
   "source": [
    "# test sequence padding\n",
    "seq_length=200\n",
    "features = pad_features(test_ints, seq_length)\n",
    "\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 200])\n"
     ]
    }
   ],
   "source": [
    "# test conversion to tensor and pass into your model\n",
    "feature_tensor = torch.from_numpy(features)\n",
    "print(feature_tensor.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(net, test_review, sequence_length=200):\n",
    "    \n",
    "    net.eval()\n",
    "    \n",
    "    # tokenize review\n",
    "    test_ints = tokenize_review(test_review)\n",
    "    \n",
    "    # pad tokenized sequence\n",
    "    seq_length=sequence_length\n",
    "    features = pad_features(test_ints, seq_length)\n",
    "    \n",
    "    # convert to tensor to pass into your model\n",
    "    feature_tensor = torch.from_numpy(features)\n",
    "    \n",
    "    batch_size = feature_tensor.size(0)\n",
    "    \n",
    "    # initialize hidden state\n",
    "    h = net.init_hidden(batch_size)\n",
    "    \n",
    "    if(use_cuda):\n",
    "        feature_tensor = feature_tensor.cuda()\n",
    "    \n",
    "    # get the output from the model\n",
    "    output, h = net(feature_tensor, h)\n",
    "    \n",
    "    # convert output probabilities to predicted class (0 or 1)\n",
    "    pred = torch.round(output.squeeze()) \n",
    "    # printing output value, before rounding\n",
    "    print('Prediction value, pre-rounding: {:.6f}'.format(output.item()))\n",
    "    \n",
    "    # print custom response\n",
    "    if(pred.item()==1):\n",
    "        print(\"Positive review detected!\")\n",
    "    else:\n",
    "        print(\"Negative review detected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# positive test review\n",
    "test_review_pos = 'This movie had the best acting and the dialogue was so good. I loved it.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction value, pre-rounding: 0.005533\n",
      "Negative review detected.\n"
     ]
    }
   ],
   "source": [
    "# call function\n",
    "seq_length=200 # good to use the length that was trained on\n",
    "\n",
    "predict(net, test_review_neg, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction value, pre-rounding: 0.972495\n",
      "Positive review detected!\n"
     ]
    }
   ],
   "source": [
    "# call function\n",
    "seq_length=200 # good to use the length that was trained on\n",
    "\n",
    "predict(net, test_review_pos, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
